{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "é€™ä»½ Notebook ç¤ºç¯„ Google Gemini API çš„ä½¿ç”¨\n",
        "\n",
        "### Google Colab Tips\n",
        "\n",
        "* ç”¨ Shift+Enter å¯ä»¥åŸ·è¡Œç¨‹å¼å€å¡Š (æˆ–æ˜¯æ»‘é¼ é»å‰é¢çš„åŸ·è¡Œç¬¦è™Ÿ)\n",
        "* å¦‚æœè¦ä¿®æ”¹å­˜æª”ï¼Œéœ€è¦å…ˆé» \"æª”æ¡ˆ\" -> \"åœ¨é›²ç«¯ç¡¬ç¢Ÿä¸­å„²å­˜å‰¯æœ¬\"ï¼Œè¤‡è£½åˆ°ä½ è‡ªå·±çš„ç›®éŒ„ä¸‹ï¼Œæ‰å¯ä»¥å„²å­˜\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## è¨­å®š Google Gemini API Key è®Šæ•¸\n",
        "\n",
        "è«‹é»é–‹å·¦å´æ¬„çš„Keyç¬¦è™Ÿï¼Œå°±å¯ä»¥è¨­å®š Secretã€‚è«‹è¨­å®š gemini_api_key\n",
        "\n",
        "è«‹åˆ° https://aistudio.google.com/app/apikey ç”³è«‹ Gemini API Key\n",
        "\n",
        "* å…è²»æ–¹æ¡ˆæœ‰ä¸€å®šçš„ä½¿ç”¨é™åˆ¶ï¼Œä½†å°æ–¼å­¸ç¿’ä¾†èªªå·²ç¶“è¶³å¤ \n",
        "* API Key è«‹å¦¥å–„ä¿ç®¡ï¼Œä¸è¦å…¬é–‹åˆ†äº«\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "gemini_api_key = userdata.get('gemini_api_key')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "from pprint import pp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ HTTP è«‹æ±‚ç›´æ¥å‘¼å« Gemini API\n",
        "payload = {\n",
        "    \"contents\": [\n",
        "        {\n",
        "            \"parts\": [\n",
        "                {\n",
        "                    \"text\": \"ä½ å¥½ï¼Œæœ€è¿‘éå¾—å¦‚ä½•?\"\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "    ],\n",
        "    \"generationConfig\": {\n",
        "        \"temperature\": 0  # å¯ä»¥æ”¹æ”¹çœ‹ æº«åº¦\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Gemini API endpoint\n",
        "url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={gemini_api_key}\"\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "obj = json.loads(response.text)\n",
        "\n",
        "pp(obj)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ä½¿ç”¨ Google AI Python SDK\n",
        "\n",
        "https://github.com/google/generative-ai-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install google-generativeai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# è¨­å®š API Key\n",
        "genai.configure(api_key=gemini_api_key)\n",
        "\n",
        "# åˆå§‹åŒ–æ¨¡å‹\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "# ç”Ÿæˆå›æ‡‰\n",
        "response = model.generate_content(\"ä½ å¥½ï¼Œæœ€è¿‘éå¾—å¦‚ä½•?\")\n",
        "\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ä½¿ç”¨ System Instruction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# é€™æ˜¯ completion é¢¨æ ¼(è »å¤šæ•™æä»é€™æ¨£å¯«)\n",
        "user_message = \"\"\"è«‹åˆ†é¡ä»¥ä¸‹æ–‡å­—æ˜¯ neutral, negative æˆ– positive\n",
        "æ–‡å­—: æˆ‘è¦ºå¾—é€™å€‹æŠ«è–©å¯¦åœ¨å¤ªå¥½åƒå•¦\n",
        "æƒ…ç·’:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(user_message)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¯æ”¹æˆä½¿ç”¨ system instruction çš„é¢¨æ ¼: æŠŠä¸è®Šçš„æ•´é«”æŒ‡ç¤ºæ”¾åœ¨ system instruction\n",
        "# user prompt æ”¾è®Šå‹•çš„ç”¨æˆ¶è¼¸å…¥\n",
        "\n",
        "# ä½¿ç”¨ system instruction å‰µå»ºæ¨¡å‹\n",
        "model_with_system = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    system_instruction=\"è«‹åˆ†é¡ä»¥ä¸‹æ–‡å­—æ˜¯ neutral, negative æˆ– positive\"\n",
        ")\n",
        "\n",
        "user_message = \"\"\"\n",
        "æ–‡å­—: æˆ‘è¦ºå¾—é€™å€‹æŠ«è–©å¯¦åœ¨å¤ªå¥½åƒå•¦\n",
        "\"\"\"\n",
        "\n",
        "response = model_with_system.generate_content(user_message)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## é€£çºŒå°è©±å¤šè¼ªå°è©±çš„å ´æ™¯ç¤ºç¯„\n",
        "\n",
        "* æ¨¡å‹æ˜¯ Stateless ç„¡ç‹€æ…‹çš„\n",
        "* æ¯æ¬¡ä½ éƒ½å¾—æŠŠæ‰€æœ‰å°è©±æ­·å²å‚³çµ¦ Gemini API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ ChatSession é€²è¡Œé€£çºŒå°è©±\n",
        "model_chat = genai.GenerativeModel('gemini-1.5-flash', \n",
        "                                   system_instruction=\"You are a helpful assistant.\")\n",
        "\n",
        "chat = model_chat.start_chat()\n",
        "\n",
        "# ç¬¬ä¸€è¼ªå•ç­”\n",
        "response1 = chat.send_message(\"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\")\n",
        "print(\"ç¬¬ä¸€è¼ªå›ç­”:\", response1.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å»¶çºŒåŒä¸€å€‹å°è©±çš„ ç¬¬äºŒè¼ªå•ç­”\n",
        "response2 = chat.send_message(\"é‚£2017å¹´å‘¢?\")\n",
        "print(\"ç¬¬äºŒè¼ªå›ç­”:\", response2.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å»¶çºŒåŒä¸€å€‹å°è©±çš„ ç¬¬ä¸‰è¼ªå•ç­”\n",
        "response3 = chat.send_message(\"ç¾åœ‹éšŠè´éå¹¾æ¬¡å† è»?\")\n",
        "print(\"ç¬¬ä¸‰è¼ªå›ç­”:\", response3.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### æ¨¡å‹çš„å¹»è¦ºç¾è±¡ Hallucination\n",
        "\n",
        "æ¯”è¼ƒè°æ˜çš„æ¨¡å‹ï¼Œå¹»è¦ºç¾è±¡æœƒæ¯”è¼ƒå°‘\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å¦‚æœ ç¬¬äºŒè¼ªå•ç­”æ™‚ æ˜¯å• 2018 å¹´\n",
        "chat_new = model_chat.start_chat()\n",
        "chat_new.send_message(\"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\")\n",
        "response4 = chat_new.send_message(\"é‚£2018å¹´å‘¢?\")\n",
        "print(\"2018å¹´å›ç­”:\", response4.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ›ä¸€ç¨®å•æ³• æ¸›å°‘å¹»è¦ºç¾è±¡\n",
        "chat_better = model_chat.start_chat()\n",
        "chat_better.send_message(\"èª°è´å¾—2013å¹´çš„ä¸–ç•Œæ£’çƒç¶“å…¸è³½å† è»?\")\n",
        "response5 = chat_better.send_message(\"é‚£2018å¹´å‘¢? å¦‚æœæ²’èˆ‰è¾¦ï¼Œè«‹å›ç­”æ²’èˆ‰è¾¦\")\n",
        "print(\"æ”¹å–„å¾Œçš„2018å¹´å›ç­”:\", response5.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Few-shot prompting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡ºè™•: https://www.promptingguide.ai/zh/techniques/fewshot\n",
        "prompt = f\"\"\"\n",
        "è«‹åˆ¤æ–·æƒ…ç·’:\n",
        "\n",
        "input: é€™å¤ªæ£’äº†ï¼\n",
        "output: Positive\n",
        "\n",
        "input: é€™å¤ªç³Ÿç³•äº†ï¼\n",
        "output: Negative\n",
        "\n",
        "input: å“‡ï¼Œé‚£éƒ¨é›»å½±å¤ªæ£’äº†ï¼\n",
        "output: Positive\n",
        "\n",
        "input: å¤šéº¼å¯æ€•çš„ç¯€ç›®\n",
        "output:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# *åœ¨ä¸€äº›è¼ƒé›£æè¿°æ˜ç¢ºæŒ‡ç¤ºçš„ä»»å‹™ä¸­ï¼Œè »é©åˆç”¨* few-shot çš„æ–¹å¼è®“æ¨¡å‹è‡ªå·±å­¸ï¼Œä¾‹å¦‚æ–‡å­—é¢¨æ ¼ã€ç‰¹å®šçš„è¼¸å‡ºçµæ§‹(æŸç¨®schema)\n",
        "\n",
        "# æ²’çµ¦ç¯„ä¾‹\n",
        "prompt = f\"\"\"\n",
        "æ™¶æ™¶é«”æ˜¯ä¸€ç¨®æµè¡Œæ–¼è‡ºç£ä»¥ä¸­æ–‡ç‚ºåŸºåº•ï¼Œå¤¾é›œè‹±èªä¸æˆå¥çš„å–®å­—æˆ–ç‰‡èªçš„è¡¨é”æ–¹å¼ã€‚ç‰¹æŒ‡æ‰€ä½¿ç”¨çš„è‹±æ–‡å­—å¤šç‚ºéæ–¼ç°¡å–®ã€æ²’æœ‰æ›¿æ›å¿…è¦è€…ï¼Œé€²è€Œç”¢ç”Ÿæœ‰æ„ç‚«è€€é›™èªèƒ½åŠ›å»å¼„å·§æˆæ‹™çš„æ•ˆæœã€‚\n",
        "\n",
        "åŸæ–‡: æ¯ä½å“¡å·¥éƒ½è¦åƒåŠ æ¯é€±é›»è©±æœƒè­°ï¼Œæ²’æœ‰ä¾‹å¤–\n",
        "æ™¶æ™¶é«”:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# çµ¦ç¯„ä¾‹è®“æ¨¡å‹å­¸é¢¨æ ¼ï¼Œå¯ä»¥å­¸å¾—æ›´å¥½\n",
        "prompt = f\"\"\"\n",
        "æ™¶æ™¶é«”æ˜¯ä¸€ç¨®æµè¡Œæ–¼è‡ºç£ä»¥ä¸­æ–‡ç‚ºåŸºåº•ï¼Œå¤¾é›œè‹±èªä¸æˆå¥çš„å–®å­—æˆ–ç‰‡èªçš„è¡¨é”æ–¹å¼ã€‚ç‰¹æŒ‡æ‰€ä½¿ç”¨çš„è‹±æ–‡å­—å¤šç‚ºéæ–¼ç°¡å–®ã€æ²’æœ‰æ›¿æ›å¿…è¦è€…ï¼Œé€²è€Œç”¢ç”Ÿæœ‰æ„ç‚«è€€é›™èªèƒ½åŠ›å»å¼„å·§æˆæ‹™çš„æ•ˆæœã€‚\n",
        "ä¾‹å¦‚:\n",
        "\n",
        "åŸæ–‡: æˆ‘å¾ˆå¿™ï¼Œå› ç‚ºæˆ‘å¾ˆæœ‰äº‹è¦åš\n",
        "æ™¶æ™¶é«”: æˆ‘æ˜¯å¾ˆbusyï¼Œå› ç‚ºæˆ‘å¾ˆå¤šthingsè¦do\n",
        "\n",
        "åŸæ–‡: å¤©æ°£ç¸½ç®—æ”¾æ™´ï¼Œæ²’æœ‰ä¸‹é›¨ã€å¤ªé™½å¾ˆå¤§ã€æœ‰é»ç†±ã€è®“æˆ‘æƒ³åˆ°ä»¥å‰é‚„æ˜¯å­¸ç”Ÿæ™‚ï¼Œå–œæ­¡åœ¨é€™æ¨£çš„å¤©æ°£ï¼Œåƒä¸€çƒå†°æ·‡æ·‹ï¼ŒçœŸçš„æœƒè®“äººå¾ˆé«˜èˆˆ\n",
        "æ™¶æ™¶é«”: å¤©æ°£ç¸½ç®—æ”¾æ™´ï¼Œæ²’æœ‰rainã€å¤ªé™½å¾ˆbigã€æœ‰é»hotã€è®“æˆ‘æƒ³åˆ°ä»¥å‰é‚„æ˜¯studentæ™‚ï¼Œå–œæ­¡åœ¨é€™æ¨£çš„å¤©æ°£ï¼Œåƒä¸€çƒice creamï¼ŒçœŸçš„æœƒè®“äººå¾ˆhappy\n",
        "\n",
        "åŸæ–‡: æ¯ä½å“¡å·¥éƒ½è¦åƒåŠ æ¯é€±é›»è©±æœƒè­°ï¼Œæ²’æœ‰ä¾‹å¤–\n",
        "æ™¶æ™¶é«”:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Chain-of-Thought (CoT) Prompting\n",
        "\n",
        "* çµ¦æ¨¡å‹æ€è€ƒæ™‚é–“\n",
        "* ç¤ºç¯„å¦‚ä½•æ‹†è§£æ­¥é©Ÿï¼Œå¥½è®“æ¨¡å‹å°ä¸€å€‹å•é¡Œé€²è¡Œæ›´é•·çš„æ€è€ƒæ™‚é–“ (ä¹Ÿå°±æ˜¯ï¼Œæœ‰æ›´å¤šçš„è¼¸å‡º)\n",
        "\n",
        "å‡ºè™• CoT Paper: https://arxiv.org/abs/2201.11903\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å‡ºè™•: https://promptingguide.azurewebsites.net/techniques/cot\n",
        "prompt = \"\"\"\n",
        "æˆ‘å»å¸‚å ´è²·äº†10å€‹è˜‹æœã€‚æˆ‘çµ¦äº†é„°å±…2å€‹è˜‹æœï¼Œåˆçµ¦ä¿®ç†å·¥2å€‹è˜‹æœã€‚ä¹‹å¾Œï¼Œæˆ‘åˆå»è²·äº†5å€‹è˜‹æœï¼Œç„¶å¾Œåƒäº†1å€‹ã€‚æˆ‘é‚„å‰©ä¸‹å¤šå°‘å€‹è˜‹æœï¼Ÿ\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Few-shot CoT - çµ¦ä¸€å€‹æ¨ç†ç¯„ä¾‹ï¼Œä¹Ÿå°±æ˜¯ Chain of Thought (CoT) æ€è€ƒéç¨‹\n",
        "prompt = \"\"\"\n",
        "Q: æˆ‘å»å¸‚å ´è²·äº†6å€‹é¦™è•‰ï¼Œçµ¦äº†æœ‹å‹3å€‹é¦™è•‰ï¼Œæˆ‘é‚„å‰©ä¸‹å¤šå°‘å€‹?\n",
        "A:\n",
        "  1. æˆ‘ä¸€é–‹å§‹æœ‰6å€‹\n",
        "  2. çµ¦äº†æœ‹å‹3å€‹ï¼Œæ‰€ä»¥å‰©ä¸‹ 6-3=3å€‹é¦™è•‰\n",
        "  3. æœ€å¾Œå‰©ä¸‹3å€‹é¦™è•‰\n",
        "\n",
        "Q: æˆ‘å»å¸‚å ´è²·äº†10å€‹è˜‹æœã€‚æˆ‘çµ¦äº†é„°å±…2å€‹è˜‹æœï¼Œåˆçµ¦ä¿®ç†å·¥2å€‹è˜‹æœã€‚ä¹‹å¾Œï¼Œæˆ‘åˆå»è²·äº†5å€‹è˜‹æœï¼Œç„¶å¾Œåƒäº†1å€‹ã€‚æˆ‘é‚„å‰©ä¸‹å¤šå°‘å€‹è˜‹æœï¼Ÿ\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero-shot CoT (è®“æ¨¡å‹è‡ªå·±æƒ³æ­¥é©Ÿ)\n",
        "# æ¨™æº–å’’èªæ˜¯ Let's think step by step\n",
        "prompt = \"\"\"\n",
        "æˆ‘å»å¸‚å ´è²·äº†10å€‹è˜‹æœã€‚æˆ‘çµ¦äº†é„°å±…2å€‹è˜‹æœï¼Œåˆçµ¦ä¿®ç†å·¥2å€‹è˜‹æœã€‚ä¹‹å¾Œï¼Œæˆ‘åˆå»è²·äº†5å€‹è˜‹æœï¼Œç„¶å¾Œåƒäº†1å€‹ã€‚æˆ‘é‚„å‰©ä¸‹å¤šå°‘å€‹è˜‹æœï¼Ÿ\n",
        "Let's think step by step\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## å¦‚ä½•åš çµæ§‹åŒ–è¼¸å‡º (JSON è¼¸å‡º) ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"è«‹éš¨æ©Ÿç”¢ç”Ÿä¸‰å€‹ user è³‡æ–™ï¼Œè«‹ç”¨ JSON æ ¼å¼å›å‚³\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gemini çš„ Structured Output åŠŸèƒ½\n",
        "# æ–‡ä»¶: https://ai.google.dev/gemini-api/docs/json-mode\n",
        "\n",
        "# å®šç¾© JSON Schema\n",
        "response_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"è«‹ç”¨å°ç£å¸¸è¦‹å§“å\"\n",
        "        },\n",
        "        \"age\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"å¹´ç´€\"\n",
        "        },\n",
        "        \"bio\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡\"\n",
        "        },\n",
        "        \"avatar_url\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"å€‹äººåœ–åƒï¼Œè«‹ç”¨çœŸå¯¦å¯ä»¥é€£çµçš„åœ–ç‰‡\"\n",
        "        },\n",
        "        \"isSubscriber\": {\n",
        "            \"type\": \"boolean\",\n",
        "            \"description\": \"æ˜¯å¦è¨‚é–±\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\n",
        "        \"name\",\n",
        "        \"age\", \n",
        "        \"bio\",\n",
        "        \"avatar_url\",\n",
        "        \"isSubscriber\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# ä½¿ç”¨ response schema çš„æ¨¡å‹\n",
        "model_structured = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema\n",
        "    )\n",
        ")\n",
        "\n",
        "response = model_structured.generate_content(prompt)\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ Pydantic å®šç¾© schema æ›´æ–¹ä¾¿\n",
        "from typing import List\n",
        "from pydantic import Field, BaseModel, ConfigDict\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str = Field(description=\"è«‹ç”¨å°ç£å¸¸è¦‹å§“å\")\n",
        "    age: int = Field(description=\"å¹´ç´€\")\n",
        "    bio: str = Field(description=\"è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡\")\n",
        "    avatar_url: str = Field(description=\"å€‹äººåœ–åƒï¼Œè«‹ç”¨çœŸå¯¦å¯ä»¥é€£çµçš„åœ–ç‰‡\")\n",
        "    isSubscriber: bool = Field(description=\"æ˜¯å¦è¨‚é–±\")\n",
        "\n",
        "class Users(BaseModel):\n",
        "    users: list[User]\n",
        "\n",
        "# å°‡ Pydantic model è½‰æ›ç‚º Gemini å¯ç”¨çš„ schema\n",
        "def pydantic_to_gemini_schema(pydantic_model):\n",
        "    schema = pydantic_model.model_json_schema()\n",
        "    return schema\n",
        "\n",
        "users_schema = pydantic_to_gemini_schema(Users)\n",
        "print(\"Generated Schema:\", users_schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ Pydantic schema å‰µå»ºæ¨¡å‹\n",
        "model_pydantic = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=users_schema\n",
        "    )\n",
        ")\n",
        "\n",
        "response = model_pydantic.generate_content(\"è«‹éš¨æ©Ÿç”¢ç”Ÿå¤šå€‹ user è³‡æ–™\")\n",
        "print(response.text)\n",
        "\n",
        "# è§£æå›æ‡‰åˆ° Pydantic ç‰©ä»¶\n",
        "import json\n",
        "response_data = json.loads(response.text)\n",
        "parsed_users = Users(**response_data)\n",
        "print(\"\\nè§£æå¾Œçš„ç¬¬ä¸€å€‹ç”¨æˆ¶:\", parsed_users.users[0])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Prompt Chaining: å·¥å…·ä¸²æ¥\n",
        "\n",
        "ä¸‰å€‹æ­¥é©Ÿ:\n",
        "\n",
        "1. å¾ç”¨æˆ¶å•é¡Œä¸­ï¼Œç”¨ prompt1 ä¾†æå–å‡º å¤–éƒ¨å·¥å…·çš„åƒæ•¸\n",
        "2. åŸ·è¡Œå¤–éƒ¨å·¥å…·ï¼Œæ‹¿åˆ°çµæœ\n",
        "3. ç”¨ (prompt2 + çµæœ) è½‰æˆè‡ªç„¶èªè¨€å›çµ¦ç”¨æˆ¶\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: å¾ç”¨æˆ¶å•é¡Œä¸­ï¼Œç”¨ prompt1 ä¾†æå–å‡º å¤–éƒ¨å·¥å…·çš„åƒæ•¸\n",
        "from typing import List\n",
        "from pydantic import Field, BaseModel\n",
        "\n",
        "class QueryResult(BaseModel):\n",
        "    date: str = Field(description=\"Date in yyyymmdd format. Leave empty if not parsable\")\n",
        "    stock_code: str = Field(description=\"Taiwan stock code as a 4-digit number. Leave empty if not parsable\")\n",
        "\n",
        "query = \"è«‹å•2025å¹´4æœˆ10è™Ÿçš„å°ç©é›»ï¼Œè‚¡åƒ¹è¡¨ç¾å¦‚ä½•?\"\n",
        "\n",
        "# å‰µå»ºå°ˆé–€ç”¨æ–¼åƒæ•¸æå–çš„æ¨¡å‹\n",
        "extract_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    system_instruction=\"Extract from user queries\",\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=pydantic_to_gemini_schema(QueryResult)\n",
        "    )\n",
        ")\n",
        "\n",
        "response = extract_model.generate_content(query)\n",
        "extracted_data = json.loads(response.text)\n",
        "parsed_result = QueryResult(**extracted_data)\n",
        "print(\"æå–çš„åƒæ•¸:\", parsed_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: åŸ·è¡Œå·¥å…·ï¼Œæ‹¿åˆ°çµæœ\n",
        "# API åƒè€ƒè‡ª https://medium.com/%E5%B7%A5%E7%A8%8B%E9%9A%A8%E5%AF%AB%E7%AD%86%E8%A8%98/5%E7%A8%AE%E6%8A%93%E5%8F%96%E5%8F%B0%E8%82%A1%E6%AD%B7%E5%8F%B2%E8%82%A1%E5%83%B9%E7%9A%84%E6%96%B9%E6%B3%95-766bf2ed9d6\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "date = parsed_result.date\n",
        "stock_code = parsed_result.stock_code\n",
        "url = 'https://www.twse.com.tw/exchangeReport/STOCK_DAY?response=json&date=%s&stockNo=%s' % (date, stock_code)\n",
        "\n",
        "html = requests.get(url)\n",
        "context = json.loads(html.text)\n",
        "print(\"å–å¾—çš„è‚¡åƒ¹è³‡æ–™:\", context)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: ç”¨ (prompt2 + çµæœ) è½‰æˆè‡ªç„¶èªè¨€å›çµ¦ç”¨æˆ¶\n",
        "prompt = f\"\"\"\n",
        "Based on the provided context, please answer the following question in Traditional Chinese (Taiwan):\n",
        "\n",
        "Question: <question>{query}</question>\n",
        "\n",
        "Context: <context>{context}</context>\n",
        "\n",
        "Instructions:\n",
        "1. Carefully verify that your answer is supported by the given context.\n",
        "2. If the question cannot be answered from the provided context, respond with: \"æŠ±æ­‰ï¼Œåœ¨æä¾›çš„è³‡æ–™ä¸­æ‰¾ä¸åˆ°ç›¸é—œè³‡è¨Šï¼Œç„¡æ³•å›ç­”æ‚¨çš„å•é¡Œã€‚\"\n",
        "3. Do not include information that is not present in the context.\n",
        "4. Ensure your response is written in Traditional Chinese as used in Taiwan.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(\"æœ€çµ‚å›ç­”:\", response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Function Calling\n",
        "\n",
        "Gemini çš„ Function Calling åŠŸèƒ½èˆ‡ OpenAI ç›¸ä¼¼ï¼Œä½†èªæ³•ç•¥æœ‰ä¸åŒ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: å®šç¾©å·¥å…·å‡½å¼\n",
        "def web_search(keyword: str) -> str:\n",
        "    \"\"\"æœå°‹æœ€æ–°çš„è³‡è¨Š\"\"\"\n",
        "    # æ¨¡æ“¬æœå°‹çµæœ\n",
        "    return f\"æœå°‹é—œéµå­— '{keyword}' çš„çµæœ: ä»Šå¤©å°åŒ—å¤©æ°£æ™´æœ—ï¼Œæ°£æº«25åº¦ï¼Œé©åˆå¤–å‡ºæ´»å‹•ã€‚\"\n",
        "\n",
        "# Step 2: å®šç¾©å‡½æ•¸æè¿°çµ¦ Gemini\n",
        "web_search_func = genai.protos.FunctionDeclaration(\n",
        "    name=\"web_search\",\n",
        "    description=\"æœå°‹æœ€æ–°çš„è³‡è¨Š\",\n",
        "    parameters=genai.protos.Schema(\n",
        "        type=genai.protos.Type.OBJECT,\n",
        "        properties={\n",
        "            \"keyword\": genai.protos.Schema(\n",
        "                type=genai.protos.Type.STRING,\n",
        "                description=\"æœå°‹é—œéµå­—\"\n",
        "            )\n",
        "        },\n",
        "        required=[\"keyword\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Step 3: å‰µå»ºåŒ…å«å·¥å…·çš„æ¨¡å‹\n",
        "model_with_tools = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    tools=[genai.protos.Tool(function_declarations=[web_search_func])]\n",
        ")\n",
        "\n",
        "# Step 4: ç™¼é€è«‹æ±‚\n",
        "from datetime import datetime\n",
        "today = datetime.now().strftime(\"%Y/%m/%d\")\n",
        "response = model_with_tools.generate_content(f\"ä»Šå¤© {today} å°åŒ—å¤©æ°£å¦‚ä½•?\")\n",
        "\n",
        "print(\"æ¨¡å‹å›æ‡‰:\")\n",
        "print(response.text)\n",
        "print(\"\\nFunction Calls:\")\n",
        "for part in response.parts:\n",
        "    if part.function_call:\n",
        "        print(f\"Function: {part.function_call.name}\")\n",
        "        print(f\"Arguments: {dict(part.function_call.args)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: è™•ç†å‡½æ•¸èª¿ç”¨ä¸¦åŸ·è¡Œ\n",
        "available_functions = {\n",
        "    \"web_search\": web_search\n",
        "}\n",
        "\n",
        "# å¦‚æœæœ‰å‡½æ•¸èª¿ç”¨ï¼ŒåŸ·è¡Œå®ƒå€‘\n",
        "if any(part.function_call for part in response.parts):\n",
        "    # æ”¶é›†å‡½æ•¸èª¿ç”¨çµæœ\n",
        "    function_responses = []\n",
        "    \n",
        "    for part in response.parts:\n",
        "        if part.function_call:\n",
        "            function_name = part.function_call.name\n",
        "            function_args = dict(part.function_call.args)\n",
        "            \n",
        "            print(f\"åŸ·è¡Œå‡½æ•¸: {function_name}({function_args})\")\n",
        "            \n",
        "            # èª¿ç”¨å¯¦éš›å‡½æ•¸\n",
        "            function_to_call = available_functions[function_name]\n",
        "            function_result = function_to_call(**function_args)\n",
        "            \n",
        "            print(f\"å‡½æ•¸çµæœ: {function_result}\")\n",
        "            \n",
        "            # æº–å‚™å‡½æ•¸å›æ‡‰\n",
        "            function_response = genai.protos.Part(\n",
        "                function_response=genai.protos.FunctionResponse(\n",
        "                    name=function_name,\n",
        "                    response={\"result\": function_result}\n",
        "                )\n",
        "            )\n",
        "            function_responses.append(function_response)\n",
        "    \n",
        "    # Step 6: å°‡å‡½æ•¸çµæœå›å‚³çµ¦æ¨¡å‹ç”Ÿæˆæœ€çµ‚å›æ‡‰\n",
        "    final_response = model_with_tools.generate_content([\n",
        "        genai.protos.Content(parts=response.parts),  # åŸå§‹å›æ‡‰\n",
        "        genai.protos.Content(parts=function_responses)  # å‡½æ•¸çµæœ\n",
        "    ])\n",
        "    \n",
        "    print(\"\\næœ€çµ‚å›æ‡‰:\")\n",
        "    print(final_response.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŒ…æˆä¸€å€‹å®Œæ•´çš„ function calling è¼”åŠ©å‡½æ•¸\n",
        "def run_gemini_with_functions(model, user_message, available_functions):\n",
        "    \"\"\"åŸ·è¡Œå¸¶æœ‰å‡½æ•¸èª¿ç”¨çš„ Gemini è«‹æ±‚\"\"\"\n",
        "    \n",
        "    print(f\"ğŸ’¬ ç”¨æˆ¶å•é¡Œ: {user_message}\")\n",
        "    \n",
        "    # ç¬¬ä¸€æ¬¡èª¿ç”¨\n",
        "    response = model.generate_content(user_message)\n",
        "    \n",
        "    # æª¢æŸ¥æ˜¯å¦æœ‰å‡½æ•¸èª¿ç”¨\n",
        "    function_calls = [part for part in response.parts if part.function_call]\n",
        "    \n",
        "    if function_calls:\n",
        "        print(\"ğŸ”§ æª¢æ¸¬åˆ°å‡½æ•¸èª¿ç”¨\")\n",
        "        \n",
        "        # åŸ·è¡Œæ‰€æœ‰å‡½æ•¸èª¿ç”¨\n",
        "        function_responses = []\n",
        "        \n",
        "        for part in response.parts:\n",
        "            if part.function_call:\n",
        "                function_name = part.function_call.name\n",
        "                function_args = dict(part.function_call.args)\n",
        "                \n",
        "                print(f\"   âš™ï¸ åŸ·è¡Œå‡½æ•¸ {function_name} åƒæ•¸ {function_args}\")\n",
        "                \n",
        "                # èª¿ç”¨å¯¦éš›å‡½æ•¸\n",
        "                function_to_call = available_functions[function_name]\n",
        "                function_result = function_to_call(**function_args)\n",
        "                \n",
        "                # æº–å‚™å‡½æ•¸å›æ‡‰\n",
        "                function_response = genai.protos.Part(\n",
        "                    function_response=genai.protos.FunctionResponse(\n",
        "                        name=function_name,\n",
        "                        response={\"result\": function_result}\n",
        "                    )\n",
        "                )\n",
        "                function_responses.append(function_response)\n",
        "        \n",
        "        # ç¬¬äºŒæ¬¡èª¿ç”¨ï¼ŒåŒ…å«å‡½æ•¸çµæœ\n",
        "        final_response = model.generate_content([\n",
        "            genai.protos.Content(parts=response.parts),\n",
        "            genai.protos.Content(parts=function_responses)\n",
        "        ])\n",
        "        \n",
        "        return final_response.text\n",
        "    else:\n",
        "        # æ²’æœ‰å‡½æ•¸èª¿ç”¨ï¼Œç›´æ¥è¿”å›\n",
        "        return response.text\n",
        "\n",
        "# æ¸¬è©¦é€™å€‹è¼”åŠ©å‡½æ•¸\n",
        "result = run_gemini_with_functions(\n",
        "    model_with_tools, \n",
        "    \"ä»Šå¤©å°åŒ—ã€æ–°ç«¹ã€é«˜é›„çš„å¤©æ°£å¦‚ä½•ï¼Ÿ\", \n",
        "    available_functions\n",
        ")\n",
        "\n",
        "print(\"------\")\n",
        "print(\"æœ€çµ‚çµæœ:\", result)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Gemini çš„å¤šæ¨¡æ…‹èƒ½åŠ›\n",
        "\n",
        "Gemini åŸç”Ÿæ”¯æ´åœ–åƒã€éŸ³è¨Šã€å½±ç‰‡ç­‰å¤šç¨®è¼¸å…¥æ ¼å¼ï¼Œé€™æ˜¯å®ƒç›¸æ¯” OpenAI çš„ä¸€å¤§å„ªå‹¢\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åœ–åƒè™•ç†ç¯„ä¾‹\n",
        "# é¦–å…ˆæˆ‘å€‘éœ€è¦ä¸Šå‚³ä¸€å€‹åœ–åƒæ–‡ä»¶åˆ° Gemini\n",
        "\n",
        "# æ–¹æ³•1: å¾ URL è¼‰å…¥åœ–åƒ (éœ€è¦ Pillow)\n",
        "%pip install Pillow\n",
        "\n",
        "import PIL.Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# ä¸‹è¼‰ä¸€å€‹ç¯„ä¾‹åœ–åƒ\n",
        "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
        "response = requests.get(image_url)\n",
        "image = PIL.Image.open(BytesIO(response.content))\n",
        "\n",
        "# ä½¿ç”¨ Gemini åˆ†æåœ–åƒ\n",
        "multimodal_model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = multimodal_model.generate_content([\n",
        "    \"è«‹è©³ç´°æè¿°é€™å¼µåœ–ç‰‡çš„å…§å®¹ï¼ŒåŒ…æ‹¬æ™¯è§€ã€é¡è‰²ã€æ°›åœç­‰ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚\",\n",
        "    image\n",
        "])\n",
        "\n",
        "print(\"åœ–åƒåˆ†æçµæœ:\")\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ç¸½çµ\n",
        "\n",
        "é€™ä»½ Notebook ç¤ºç¯„äº† Google Gemini API çš„ä¸»è¦åŠŸèƒ½ï¼š\n",
        "\n",
        "1. **åŸºæœ¬ä½¿ç”¨** - ç›´æ¥ HTTP è«‹æ±‚å’Œ Python SDK\n",
        "2. **System Instructions** - ç›¸ç•¶æ–¼ OpenAI çš„ System Message\n",
        "3. **å¤šè¼ªå°è©±** - ä½¿ç”¨ ChatSession\n",
        "4. **Prompt æŠ€è¡“** - Few-shot, Chain-of-Thought ç­‰\n",
        "5. **çµæ§‹åŒ–è¼¸å‡º** - JSON Schema æ”¯æ´\n",
        "6. **Prompt Chaining** - å·¥å…·ä¸²æ¥\n",
        "7. **Function Calling** - å‡½æ•¸èª¿ç”¨åŠŸèƒ½\n",
        "8. **å¤šæ¨¡æ…‹èƒ½åŠ›** - åœ–åƒã€éŸ³è¨Šã€å½±ç‰‡è™•ç†\n",
        "\n",
        "### Gemini vs OpenAI ä¸»è¦å·®ç•°\n",
        "\n",
        "**Gemini å„ªå‹¢:**\n",
        "- åŸç”Ÿå¤šæ¨¡æ…‹æ”¯æ´\n",
        "- æ›´é•·çš„ context window\n",
        "- å…è²»é¡åº¦è¼ƒå¤§æ–¹\n",
        "- Google ç”Ÿæ…‹ç³»æ•´åˆ\n",
        "\n",
        "**OpenAI å„ªå‹¢:**\n",
        "- ç”Ÿæ…‹ç³»æ›´æˆç†Ÿï¼ˆå·¥å…·ã€æ¡†æ¶ï¼‰\n",
        "- API ç©©å®šæ€§è¼ƒé«˜\n",
        "- ç¤¾ç¾¤è³‡æºè±å¯Œ\n",
        "- æ¨ç†èƒ½åŠ›é€šå¸¸è¼ƒå¼·\n",
        "\n",
        "é¸æ“‡å“ªå€‹å¹³å°æ‡‰è©²æ ¹æ“šæ‚¨çš„å…·é«”éœ€æ±‚ã€é ç®—å’ŒæŠ€è¡“è¦æ±‚ä¾†æ±ºå®šã€‚\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
